"""
Demonstration script for the enhanced AI caching system and cost tracking.

This script showcases the key features and capabilities of the integrated
caching and cost tracking system for the Spectral Analyzer.
"""

import asyncio
import logging
import sys
import time
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from config.settings import ConfigManager, CacheSettings, AISettings
from utils.cache_manager import CacheManager, CacheConfig
from utils.cost_tracker import CostTracker, Period
from core.ai_normalizer import NormalizationPlan, ColumnMapping, ConfidenceLevel


def create_sample_data():
    """Create sample spectral data for demonstration."""
    data = {
        'Wavenumber_cm1': [4000, 3500, 3000, 2500, 2000, 1500, 1000, 500],
        'Absorbance_AU': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],
        'Sample_ID': ['DEMO_001'] * 8,
        'Measurement_Date': ['2024-01-15'] * 8
    }
    return pd.DataFrame(data)


def create_sample_plan(file_hash: str) -> NormalizationPlan:
    """Create a sample normalization plan."""
    column_mappings = [
        ColumnMapping(
            original_name="Wavenumber_cm1",
            target_name="wavenumber",
            data_type="numeric",
            confidence=0.95,
            notes="High confidence wavenumber identification"
        ),
        ColumnMapping(
            original_name="Absorbance_AU",
            target_name="absorbance",
            data_type="numeric",
            confidence=0.92,
            notes="High confidence absorbance identification"
        ),
        ColumnMapping(
            original_name="Sample_ID",
            target_name="metadata",
            data_type="text",
            confidence=0.85,
            notes="Sample identifier metadata"
        )
    ]
    
    return NormalizationPlan(
        file_hash=file_hash,
        column_mappings=column_mappings,
        data_transformations=["sort_by_wavenumber_desc", "remove_duplicate_wavenumbers"],
        confidence_score=91.0,
        confidence_level=ConfidenceLevel.HIGH,
        issues_detected=[],
        metadata={
            "analysis_notes": "Standard FTIR spectroscopy format detected",
            "model_used": "x-ai/grok-4-fast"
        },
        ai_model="x-ai/grok-4-fast",
        timestamp=datetime.now().isoformat()
    )


async def demonstrate_caching_system():
    """Demonstrate the enhanced caching system capabilities."""
    print("\n" + "="*60)
    print("ðŸš€ SPECTRAL ANALYZER - AI CACHING SYSTEM DEMO")
    print("="*60)
    
    # Initialize components
    print("\n1. Initializing Enhanced Caching System...")
    
    config_manager = ConfigManager()
    config_manager.cache_settings = CacheSettings(
        enable_caching=True,
        memory_limit_mb=100,
        disk_limit_mb=500,
        default_ttl_hours=24,
        compression_enabled=True,
        enable_redis=False
    )
    
    cache_config = CacheConfig(
        memory_limit_mb=100,
        disk_limit_mb=500,
        default_ttl_hours=24,
        enable_redis=False,
        enable_background_cleanup=False
    )
    
    cache_manager = CacheManager(config=cache_config)
    cost_tracker = CostTracker(config_manager=config_manager)
    
    print("âœ“ Cache Manager initialized with multi-tier storage")
    print("âœ“ Cost Tracker initialized with persistent storage")
    print("âœ“ Configuration loaded successfully")
    
    # Demonstrate file structure hashing
    print("\n2. Demonstrating Intelligent File Structure Hashing...")
    
    sample_data = create_sample_data()
    file_hash = cache_manager.generate_file_structure_hash(sample_data, "demo_file.csv")
    
    print(f"âœ“ Generated file structure hash: {file_hash[:16]}...")
    print("âœ“ Hash is based on column structure, data types, and sample values")
    
    # Test hash consistency
    same_structure_hash = cache_manager.generate_file_structure_hash(sample_data, "different_name.csv")
    assert file_hash == same_structure_hash
    print("âœ“ Hash consistency verified - same structure produces same hash")
    
    # Demonstrate caching workflow
    print("\n3. Demonstrating Caching Workflow...")
    
    # First request - cache miss
    print("ðŸ“¤ First request (cache miss)...")
    start_time = time.time()
    
    cached_plan = await cache_manager.get_normalization_plan(file_hash)
    miss_time = (time.time() - start_time) * 1000
    
    assert cached_plan is None
    print(f"âœ“ Cache miss detected ({miss_time:.2f}ms)")
    
    # Simulate AI API call and track cost
    api_start = time.time()
    sample_plan = create_sample_plan(file_hash)
    api_time = time.time() - api_start
    
    # Track the API call cost
    cost_tracker.track_api_call(
        model="x-ai/grok-4-fast",
        provider="openrouter",
        tokens_used=1500,
        cost=0.0015,
        response_time=api_time,
        success=True,
        cache_hit=False,
        operation_type="normalization",
        file_hash=file_hash
    )
    
    print(f"âœ“ Simulated AI API call (${0.0015:.4f}, 1500 tokens)")
    
    # Store in cache
    store_success = await cache_manager.store_normalization_plan(file_hash, sample_plan)
    assert store_success
    print("âœ“ Normalization plan stored in multi-tier cache")
    
    # Second request - cache hit
    print("ðŸ“¥ Second request (cache hit)...")
    start_time = time.time()
    
    cached_plan = await cache_manager.get_normalization_plan(file_hash)
    hit_time = (time.time() - start_time) * 1000
    
    assert cached_plan is not None
    assert cached_plan.file_hash == file_hash
    print(f"âœ“ Cache hit successful ({hit_time:.2f}ms)")
    
    # Track cache hit (no cost)
    cost_tracker.track_api_call(
        model="x-ai/grok-4-fast",
        provider="openrouter",
        tokens_used=0,
        cost=0.0,
        response_time=hit_time / 1000,
        success=True,
        cache_hit=True,
        operation_type="normalization",
        file_hash=file_hash
    )
    
    print("âœ“ Cache hit tracked with zero cost")
    
    # Demonstrate cost tracking
    print("\n4. Demonstrating Cost Tracking...")
    
    session_stats = cost_tracker.get_usage_statistics(Period.SESSION)
    
    print(f"ðŸ“Š Session Statistics:")
    print(f"   â€¢ Total API calls: {session_stats.api_calls}")
    print(f"   â€¢ Cache hits: {session_stats.cache_hits}")
    print(f"   â€¢ Cache hit rate: {session_stats.cache_hit_rate:.1f}%")
    print(f"   â€¢ Total cost: ${session_stats.total_cost:.4f}")
    print(f"   â€¢ Cache savings: ${session_stats.cache_savings:.4f}")
    print(f"   â€¢ Most used model: {session_stats.most_used_model}")
    
    # Demonstrate cache statistics
    print("\n5. Demonstrating Cache Performance Metrics...")
    
    cache_stats = await cache_manager.get_cache_statistics()
    
    print(f"ðŸ“ˆ Cache Performance:")
    print(f"   â€¢ Total requests: {cache_stats.total_requests}")
    print(f"   â€¢ Cache hits: {cache_stats.cache_hits}")
    print(f"   â€¢ Hit rate: {cache_stats.hit_rate:.1f}%")
    print(f"   â€¢ Average lookup time: {cache_stats.avg_lookup_time_ms:.2f}ms")
    print(f"   â€¢ Memory usage: {cache_stats.memory_usage_mb:.2f} MB")
    print(f"   â€¢ Disk usage: {cache_stats.disk_usage_mb:.2f} MB")
    print(f"   â€¢ Total entries: {cache_stats.total_entries}")
    
    # Demonstrate multiple file types
    print("\n6. Demonstrating Multi-File Caching...")
    
    # Create different file structures
    file_types = [
        {"name": "FTIR_Standard", "cols": ["Wavenumber", "Absorbance"]},
        {"name": "Raman_Data", "cols": ["Shift_cm1", "Intensity", "Baseline"]},
        {"name": "UV_Vis", "cols": ["Wavelength_nm", "Transmittance_pct"]},
        {"name": "NIR_Spectrum", "cols": ["Frequency", "Reflectance", "Sample_Temp"]}
    ]
    
    for file_type in file_types:
        # Create sample data with different structure
        data = {}
        for i, col in enumerate(file_type["cols"]):
            data[col] = [100 + i*10 + j for j in range(8)]
        
        df = pd.DataFrame(data)
        file_hash = cache_manager.generate_file_structure_hash(df, f"{file_type['name']}.csv")
        
        # Create and store plan
        plan = create_sample_plan(file_hash)
        plan.metadata["file_type"] = file_type["name"]
        
        await cache_manager.store_normalization_plan(file_hash, plan)
        
        # Simulate API cost
        cost_tracker.track_api_call(
            model="x-ai/grok-4-fast",
            provider="openrouter",
            tokens_used=1200,
            cost=0.0012,
            response_time=0.8,
            success=True,
            cache_hit=False,
            operation_type="normalization"
        )
        
        print(f"âœ“ Cached normalization plan for {file_type['name']} format")
    
    # Test cache hits for similar structures
    print("\n7. Testing Cache Hits for Similar Structures...")
    
    for file_type in file_types[:2]:  # Test first two
        # Create similar data (same structure, different values)
        data = {}
        for i, col in enumerate(file_type["cols"]):
            data[col] = [200 + i*15 + j for j in range(8)]  # Different values
        
        df = pd.DataFrame(data)
        file_hash = cache_manager.generate_file_structure_hash(df, f"{file_type['name']}_similar.csv")
        
        # Should get cache hit
        start_time = time.time()
        cached_plan = await cache_manager.get_normalization_plan(file_hash)
        lookup_time = (time.time() - start_time) * 1000
        
        if cached_plan:
            print(f"âœ“ Cache hit for similar {file_type['name']} structure ({lookup_time:.2f}ms)")
            
            # Track cache hit
            cost_tracker.track_api_call(
                model="x-ai/grok-4-fast",
                provider="openrouter",
                tokens_used=0,
                cost=0.0,
                response_time=lookup_time / 1000,
                success=True,
                cache_hit=True,
                operation_type="normalization"
            )
        else:
            print(f"âš  Cache miss for {file_type['name']} (structure may be different)")
    
    # Final statistics
    print("\n8. Final Performance Summary...")
    
    final_session_stats = cost_tracker.get_usage_statistics(Period.SESSION)
    final_cache_stats = await cache_manager.get_cache_statistics()
    
    print(f"ðŸŽ¯ Final Results:")
    print(f"   â€¢ Total operations: {final_session_stats.api_calls}")
    print(f"   â€¢ Cache hit rate: {final_session_stats.cache_hit_rate:.1f}%")
    print(f"   â€¢ Total cost: ${final_session_stats.total_cost:.4f}")
    print(f"   â€¢ Cache savings: ${final_session_stats.cache_savings:.4f}")
    print(f"   â€¢ Cost reduction: {(final_session_stats.cache_savings / (final_session_stats.total_cost + final_session_stats.cache_savings) * 100):.1f}%")
    print(f"   â€¢ Average lookup time: {final_cache_stats.avg_lookup_time_ms:.2f}ms")
    
    # Export demonstration
    print("\n9. Demonstrating Export Capabilities...")
    
    export_path = Path("demo_usage_report.csv")
    export_success = cost_tracker.export_usage_report(str(export_path), format='csv')
    
    if export_success and export_path.exists():
        print(f"âœ“ Usage report exported to {export_path}")
        print(f"   â€¢ File size: {export_path.stat().st_size} bytes")
    else:
        print("âš  Export demonstration skipped (file system limitations)")
    
    # Cleanup demonstration
    print("\n10. Demonstrating Cache Cleanup...")
    
    cleanup_count = await cache_manager.cleanup_expired_entries()
    print(f"âœ“ Cache cleanup completed ({cleanup_count} entries processed)")
    
    cache_size_info = cache_manager.get_cache_size_info()
    print(f"âœ“ Cache size: {cache_size_info.get('disk_usage_mb', 0):.2f} MB disk, {cache_size_info.get('memory_entries', 0)} memory entries")
    
    return final_session_stats, final_cache_stats


def demonstrate_cost_alerts():
    """Demonstrate cost alert system."""
    print("\n" + "="*60)
    print("ðŸ’° COST ALERT SYSTEM DEMONSTRATION")
    print("="*60)
    
    # Create cost tracker with low limits for demo
    config_manager = ConfigManager()
    config_manager.ai_settings.cost_limit_daily = 0.01  # Very low for demo
    config_manager.ai_settings.cost_limit_monthly = 0.05
    
    cost_tracker = CostTracker(config_manager=config_manager)
    cost_tracker.daily_limit = 0.01
    cost_tracker.monthly_limit = 0.05
    
    print("1. Setting up low cost limits for demonstration...")
    print(f"   â€¢ Daily limit: ${cost_tracker.daily_limit:.3f}")
    print(f"   â€¢ Monthly limit: ${cost_tracker.monthly_limit:.3f}")
    
    # Track expensive API call
    print("\n2. Simulating expensive API call...")
    cost_tracker.track_api_call(
        model="expensive-premium-model",
        provider="openrouter",
        tokens_used=5000,
        cost=0.025,  # Exceeds both limits
        response_time=3.0,
        success=True,
        cache_hit=False,
        operation_type="complex_analysis"
    )
    
    print("âœ“ Expensive API call tracked ($0.025, 5000 tokens)")
    
    # Check for alerts
    print("\n3. Checking for cost alerts...")
    alerts = cost_tracker.get_recent_alerts(hours=1)
    
    if alerts:
        print(f"ðŸš¨ {len(alerts)} cost alerts generated:")
        for alert in alerts:
            severity_icon = "ðŸ”´" if alert.severity == "critical" else "ðŸŸ¡"
            print(f"   {severity_icon} {alert.alert_type.value.upper()}: {alert.message}")
    else:
        print("â„¹ï¸ No alerts generated (limits may not be configured)")
    
    # Show usage statistics
    stats = cost_tracker.get_usage_statistics(Period.SESSION)
    print(f"\nðŸ“Š Current Usage:")
    print(f"   â€¢ Total cost: ${stats.total_cost:.4f}")
    print(f"   â€¢ API calls: {stats.api_calls}")
    print(f"   â€¢ Average cost per call: ${stats.average_cost_per_call:.4f}")
    
    return len(alerts) > 0


def print_system_capabilities():
    """Print comprehensive system capabilities summary."""
    print("\n" + "="*60)
    print("ðŸŽ¯ SYSTEM CAPABILITIES SUMMARY")
    print("="*60)
    
    capabilities = [
        ("Multi-Tier Caching", [
            "Memory cache with LRU eviction",
            "Compressed file-based persistence",
            "Optional Redis for distributed deployment",
            "Intelligent file structure hashing",
            "Background cleanup and maintenance"
        ]),
        ("Cost Tracking", [
            "Real-time API usage monitoring",
            "Comprehensive cost calculation",
            "Budget alerts and warnings",
            "Cache savings calculation",
            "Usage analytics and forecasting"
        ]),
        ("Performance Optimization", [
            "Expected 60%+ cache hit rate",
            "<50ms cache lookup times",
            "Automatic compression for large entries",
            "Smart eviction policies",
            "Non-blocking background operations"
        ]),
        ("User Interface", [
            "Real-time cost display in status bar",
            "Comprehensive monitoring dialog",
            "Usage history and analytics",
            "Export capabilities (CSV/JSON/Excel)",
            "Alert management and configuration"
        ]),
        ("Integration", [
            "Seamless AI normalizer integration",
            "API client cost tracking",
            "Configuration management",
            "Error handling and fallbacks",
            "Comprehensive testing suite"
        ])
    ]
    
    for category, features in capabilities:
        print(f"\nðŸ”§ {category}:")
        for feature in features:
            print(f"   âœ“ {feature}")
    
    print(f"\nðŸ† Expected Benefits:")
    print(f"   ðŸ’° 50%+ cost reduction through intelligent caching")
    print(f"   âš¡ <50ms response times for cached requests")
    print(f"   ðŸ“Š Complete transparency in AI service usage")
    print(f"   ðŸ›¡ï¸ Proactive cost monitoring and alerts")
    print(f"   ðŸš€ Production-ready performance and reliability")


async def main():
    """Main demonstration function."""
    try:
        print("ðŸ”¬ Spectral Analyzer - Enhanced AI Caching & Cost Tracking System")
        print("Developed for MRG Labs - Schneider Prize Technology Innovation 2025")
        
        # Run caching demonstration
        session_stats, cache_stats = await demonstrate_caching_system()
        
        # Run cost alerts demonstration
        alerts_generated = demonstrate_cost_alerts()
        
        # Print capabilities summary
        print_system_capabilities()
        
        # Final summary
        print("\n" + "="*60)
        print("ðŸŽ‰ DEMONSTRATION COMPLETED SUCCESSFULLY")
        print("="*60)
        
        print(f"âœ… Cache System: {cache_stats.total_entries} entries, {cache_stats.hit_rate:.1f}% hit rate")
        print(f"âœ… Cost Tracking: ${session_stats.total_cost:.4f} total, ${session_stats.cache_savings:.4f} saved")
        print(f"âœ… Performance: {cache_stats.avg_lookup_time_ms:.2f}ms average lookup time")
        print(f"âœ… Alerts: {'Functional' if alerts_generated else 'Ready'}")
        print(f"âœ… Integration: All components working seamlessly")
        
        print(f"\nðŸš€ The enhanced AI caching system and cost tracking solution is")
        print(f"   ready for production deployment with significant cost optimization")
        print(f"   and transparent monitoring capabilities!")
        
        return True
        
    except Exception as e:
        logger.error(f"Demonstration failed: {e}")
        print(f"\nâŒ Demonstration failed: {e}")
        return False


if __name__ == "__main__":
    print("Starting AI Caching System and Cost Tracking Demonstration...")
    
    try:
        success = asyncio.run(main())
        
        if success:
            print("\nâœ¨ Demonstration completed successfully!")
            sys.exit(0)
        else:
            print("\nðŸ’¥ Demonstration failed!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Demonstration interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"\nðŸ’¥ Unexpected error: {e}")
        sys.exit(1)